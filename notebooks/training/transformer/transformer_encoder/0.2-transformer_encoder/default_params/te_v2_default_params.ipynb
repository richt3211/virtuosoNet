{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.experiments.training.training import init_training_job, get_dev_data, get_full_data, start_training, plot_loss\n",
    "from src.models.Transformer import TransformerEncoder, TransformerEncoderHyperParams\n",
    "from src.experiments.training.Transformer.TransformerEncoder_training import TransformerEncoderJob, TransformerEncoderJobParams\n",
    "from src.models.model_run_job import ModelJob, ModelJobParams\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_name = 'decreased_learning_rate'\n",
    "model_name = \"Transformer Encoder\"\n",
    "description = 'Decreased the learning rate and changed from adam to adamw optimizer. Also changed the weight initialization'\n",
    "is_dev=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to initalize and create the experiment as well as log the parameters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_training_job(is_dev,folder_name, model_name, description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyper_params = TransformerEncoderHyperParams()\n",
    "job_params = TransformerEncoderJobParams(is_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data = get_full_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "training_loss, valid_loss = start_training(\n",
    "    data=full_data,\n",
    "    version=0.2,\n",
    "    num_epochs=20,\n",
    "    job=TransformerEncoderJob,\n",
    "    job_params=job_params,\n",
    "    model_class=TransformerEncoder,\n",
    "    model_hyper_params=hyper_params\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss(training_loss, valid_loss, folder_name, is_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dev Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_name = 'Transformer Encoder First go'\n",
    "exp_description = 'Integrating with neptune AI for the first time'\n",
    "tags = ['transformer_encoder']\n",
    "is_dev = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyper_params = TransformerEncoderHyperParams()\n",
    "job_params = TransformerEncoderJobParams(is_dev=is_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_size': 78, 'output_size': 11, 'num_head': 6, 'hidden_size': 128, 'num_layers': 5, 'dropout': 0.1}\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import asdict\n",
    "hyper = asdict(hyper_params)\n",
    "job = asdict(job_params)\n",
    "print(hyper)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"input_size\": 78,\n",
      "    \"output_size\": 11,\n",
      "    \"num_head\": 6,\n",
      "    \"hidden_size\": 128,\n",
      "    \"num_layers\": 5,\n",
      "    \"dropout\": 0.1,\n",
      "    \"qpm_index\": 0,\n",
      "    \"vel_param_idx\": 0,\n",
      "    \"dev_param_idx\": 2,\n",
      "    \"articul_param_idx\": 3,\n",
      "    \"pedal_param_idx\": 4,\n",
      "    \"time_steps\": 500,\n",
      "    \"num_key_augmentation\": 1,\n",
      "    \"batch_size\": 1,\n",
      "    \"num_tempo_param\": 1,\n",
      "    \"num_prime_param\": 11,\n",
      "    \"device_num\": 1,\n",
      "    \"is_dev\": true,\n",
      "    \"learning_rate\": 3e-05,\n",
      "    \"grad_clip\": 0.5,\n",
      "    \"model_name\": \"TRANSFORMER ENCODER ONLY\"\n",
      "}\n",
      "https://ui.neptune.ai/richt3211/thesis/e/THESIS-11\n"
     ]
    }
   ],
   "source": [
    "exp = init_training_job(\n",
    "    is_dev=is_dev,\n",
    "    exp_name=exp_name,\n",
    "    exp_description=exp_description,\n",
    "    hyper_params=hyper_params,\n",
    "    job_params=job_params,\n",
    "    tags=tags\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_data = get_dev_data(exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "start_training() missing 1 required positional argument: 'model_folder'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-0c1f4147caa9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mmodel_class\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTransformerEncoder\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mmodel_hyper_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhyper_params\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mexp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m )\n",
      "\u001b[0;31mTypeError\u001b[0m: start_training() missing 1 required positional argument: 'model_folder'"
     ]
    }
   ],
   "source": [
    "training_loss, valid_loss = start_training(\n",
    "    data=dev_data,\n",
    "    version=0.2,\n",
    "    num_epochs=20,\n",
    "    job=TransformerEncoderJob,\n",
    "    job_params=job_params,\n",
    "    model_class=TransformerEncoder,\n",
    "    model_hyper_params=hyper_params,\n",
    "    exp=exp\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss(training_loss, valid_loss, folder_name, is_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.experiments.training.training import init_training_job, get_dev_data, get_full_data, start_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
